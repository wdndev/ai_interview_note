# 1.ML概念

#### 1.介绍一个最熟悉的机器学习算法

#### 2.机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？

- 偏差：模型无法表达数据集的复杂度，模型不够复杂，导致不能学习到基本关系，导致欠拟合；
- 方差：数据量有限，模型对数据过度敏感，导致方差；
- 过拟合容易导致高方差，欠拟合容易导致高偏差；

#### 3.如何解决过拟合问题？哪些角度

- 数据层面：更多数据、数据增强；
- 模型层面：更简单模型、更优化的模型；
- 正则化：权重衰减正则化；
- bagging等集成学习方法，深度学习中的dropout；
- 早停：训练集最优不一定在测试集最优
- BN、LN等实践中也有助于降低过拟合风险；

#### 5.说一下SVD怎么降维

SVD（奇异值分解）进行降维的基本思路是**保留矩阵中最重要的信息部分，同时丢弃那些次要的或冗余的信息，从而达到简化数据的目的**。下面我将详细介绍如何利用SVD来进行降维。

SVD是一种可以应用于任何大小矩阵的技术，它将一个矩阵分解为三个矩阵的乘积。对于一个m×n的矩阵A，它的SVD形式可以写为：

$$
[ A = U \Sigma V^T ]
$$

- $U$ ：是一个`m×m`的正交矩阵，包含了A的左奇异向量。
- $\Sigma$ ：是一个`m×n`的对角矩阵，对角线上的元素是A的奇异值，按从大到小排序。
- $V$ ：是一个`n×n`的正交矩阵，包含了A的右奇异向量。

降维的核心**在于选取**$\Sigma$\*\* 矩阵中的前k个最大的奇异值（k < n）\*\*，这样可以保留矩阵A最重要的k个方向的信息。通过这种方式，可以将原本的n维数据映射到k维空间，实现降维的目的。

假设我们要将矩阵A降维到k维空间：

1. **计算SVD**：首先计算矩阵A的SVD分解，得到( $A = U \Sigma V^T$ )。
2. **截断奇异值矩阵**：从($ \Sigma$ )矩阵中选取前k个最大的奇异值构成一个新的k阶对角矩阵( $\Sigma_k $)，这通常意味着保留( $\Sigma $)的前k行和前k列。
3. **选取主成分**：取( $U$ )矩阵的前k列构成新的矩阵( $U_k$ )，这实际上是从原始数据空间中选择了k个方向作为新的特征轴。
4. **重构矩阵**：利用( $U_k$ )和( $\Sigma_k$ )来重构一个近似矩阵( $A_{approx}$ )，这个矩阵是在k维空间内的最佳近似。有时也会直接使用( $U_k \Sigma_k$ )来表示降维后的数据，这样就可以得到一个m×k的矩阵，每个样本在这个新空间中的表示。
5. **降维后的应用**：降维后的数据可以用在多种场景中，如可视化、提高模型训练效率、减少过拟合风险等。

#### 6.推导softmax做激活函数求导

#### 7.正则化项L1和L2为什么有用

- **L1正则化**，相当于为模型添加了一个先验知识，就是**权重矩阵W服从均值为0的拉普拉斯分布**；
- **L2正则化**，相当于为模型添加了一个正态分布先验知识，就是**权重矩阵W服从均值为0的正态分布**。
- **L1正则化和L2正则化可以看做是损失函数的惩罚项**。所谓『惩罚』是指对损失函数中的某些参数做一些限制。拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。

#### 8.L1正则不可导，如何优化

在不可导处无法进行梯度下降，此时采用坐标轴下降法：坐标轴下降法是沿着坐标轴的方向，每次固定m-1个数值，对最后一个数值求局部最优解，迭代m次（证明：凸函数在每一个维度都取得最小值，则此处就是全局最小值）；

#### 9.什么样的特征容易产生比较小的权重

特征的权重反映了该特征对于模型预测结果的重要性。较小的权重可能意味着该特征对于模型输出的影响较弱，可能是由于多种原因造成的。以下是一些可能导致特征产生较小权重的因素：

1. **特征的稀有性**：当一个特征在数据集中很少出现时，模型可能会给这个特征分配较小的权重。例如，在推荐系统中，如果某个用户只看过一次某种类型的电影，而其他用户都没有看过这种类型的电影，那么在训练模型时，这种类型的电影可能会被赋予较低的权重。
2. **特征的相关性**：如果一个特征与其他特征高度相关，那么模型可能会倾向于选择那些更具代表性的特征，并给其余相关特征分配较小的权重。这是因为高度相关的特征提供的信息可能是冗余的，因此不需要太高的权重来表达它们的重要性。
3. **特征的噪声水平**：如果一个特征包含大量的噪声，那么模型可能会自动调整权重以忽略这部分特征，从而减少噪声对预测结果的影响。这种情况下，特征的权重会变得较小。
4. **特征的无关性**：如果一个特征对于预测目标几乎没有贡献，那么模型会自然地给这个特征分配一个很小甚至接近于零的权重。这样的特征可能不会帮助模型提高预测准确率。
5. **模型的正则化**：使用正则化技术（如L1或L2正则化）可以帮助减少模型的复杂度并防止过拟合。在L1正则化下，模型倾向于将不重要的特征权重压缩至零，而在L2正则化下，不重要的特征权重可能会变得非常小。
6. **训练数据的数量**：在数据量较少的情况下，模型可能会对某些特征赋予较小的权重，因为缺乏足够的证据证明这些特征的重要性。
7. **特征的动态范围**：如果一个特征的动态范围（即最大值与最小值之间的差距）很小，那么这个特征的权重也可能会较小，因为即使在数值上发生改变，对模型输出的影响也不大。
8. **模型训练算法**：不同的训练算法可能对特征权重有不同的影响。例如，在使用随机梯度下降(SGD)进行模型更新时，相比批量更新(batch update)，更容易产生大量小权重的特征。

总之，特征的权重受到多种因素的影响，包括但不限于特征本身的质量、与目标变量的关系、数据集的特性以及所使用的模型训练方法等。为了更好地理解和优化模型，分析特征权重是非常有用的步骤之一。
