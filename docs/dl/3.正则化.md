# 3.正则化

> 文章来源：[Index - 笔记 (mingchao.wang)](https://mingchao.wang/Mt7oEh5j/ "Index - 笔记 (mingchao.wang)")

1. 正则化常用于缓解模型过拟合。过拟合发生的原因是模型的容量过大，而正则化可以对模型施加某些限制，从而降低模型的有效容量。
2. 目前有多种正则化策略。
   - 有些正则化策略是向模型添加额外的约束，如增加对参数的限制。这是对参数的硬约束。
   - 有些正则化策略是向目标函数增加额外项。这是对参数的软约束。
3. 正则化策略代表了某种先验知识，即：倾向于选择简单的模型。
4. 在深度学习中，大多数正则化策略都是基于对参数进行正则化。正则化以偏差的增加来换取方差的减少，而一个有效的正则化能显著降低方差，并且不会过度增加偏差。
5. 在深度学习的实际应用中，不要因为害怕过拟合而采用一个小模型，推荐采用一个大模型并使用正则化。

## 1.参数约束正则化：L1和L2正则化

参数约束正则化主要是指 L1 和 L2 正则化；

- **L1正则化**：给每个权重减去一个与$sign(\theta_i)$ 同符号的常数因子；
- **L2正则化**（权重衰减）：给每个权重值乘上一个常数因子，线性的缩放每个权重值；

### 1.1 L2正则化

正则化项为 $\Omega(\theta)=\frac{1}{2}\|\theta\|_{2}^{2}$，系数 1/2 是为了求导时得到得系数为1；$L_2$\*\*正则化能够使参数 \*\*$\theta$**的方差更接近与0**；

目标函数：$J(\theta)=L(\theta)+\lambda \Omega(\theta)=L(\theta)+\frac{1}{2} \lambda\|\theta\|_{2}^{2}$

梯度：$\nabla_{\theta} J(\theta)=\nabla_{\theta} L(\theta)+\lambda \theta$

梯度下降过程，其中$\gamma$为学习率：$\theta \leftarrow \theta-\gamma\left(\nabla_{\theta} L(\theta)+\lambda \theta\right)$

整理得：$\theta \leftarrow(1-\gamma \lambda) \theta-\gamma \nabla_{\theta} L(\theta)$

L2正则化对梯度更新的影响是：每一步执行更新前，会对权重向量乘以一个常数因子来收缩权重向量，使参数$\theta$的方差更接近0，因此L2也被称为权重衰减。

### 1.2 L1正则化

正则化项为 $\Omega(\theta)=\|\theta\|_{1}=\sum_{i=1}^{d}\left|\theta_{i}\right|$，即各个参数的绝对值之和；

目标函数：$J(\theta)=L(\theta)+\Omega(\theta)=L(\theta)+\lambda\|\theta\|_{1}$

梯度：$\nabla_{\theta} J(\theta)=\nabla_{\theta} L(\theta)+\lambda \cdot \operatorname{sign}(\theta)$

其中 $sign(\cdot)$ 表示取自变量的符号

梯度下降过程，其中$\gamma$为学习率：$\theta \leftarrow \theta-\gamma \cdot\left(\nabla_{\theta} L(\theta)+\lambda \cdot \operatorname{sign}(\theta)\right)$

整理得：$\theta \leftarrow(\theta-\gamma \cdot \lambda \cdot \operatorname{sign}(\theta))-\gamma \cdot \nabla_{\theta} L(\theta)$

> 特别说明：对于带有$L_1$正则的目标函数，由于其不是处处可导，所以一般不使用梯度下降法进行求解，这里只是和$ L_2$正则做一个类似的讨论。在本文的最后一部分会介绍坐标轴下降法，是更常用的用于求解带有$L_1$正则的目标函数的方法；

## 2.Dropout

1. **dropout**：在前向传播过程中，对网络中的每个隐藏层，每个隐单元都以一定的概率 $p_{drop}$ 被删除，之后得到一个比原始网络要小的裁剪网络。**在反向传播过程中，仅仅对该裁剪网络进行梯度更新，被删除的隐单元不进行梯度更新**；
2. **对隐单元的删除指的是**：将该隐单元的输出置为0；当其输出为0时，该隐单元对后续神经元的影响均为0；
3. **输入层和输出层的神经元不会被删除**，这两层的神经元的个数是固定的；
4. 关于隐单元删除时的一些细节：
   - 不同的样本，其删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的；
   - 不同的样本，每个隐单元被删除的概率是相同的；
   - 同一条样本，分两次进入模型训练，那么这两次删除的隐单元也是不同的；
   - 在每个梯度更新周期中，被删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的；
   - 在每个梯度更新周期中，隐单元被删除的概率是相同的；
5. **dropout仅仅用于神经网络的训练阶段，在推理阶段不需要删除隐单元，而是使用所有的神经元**；
6. dropout的优点：
   - 其不限神经网络的网络结构，都可以使用该技术；
   - 其计算非常方便、高效；具体计算过程为：每次产生 n 个随机的二进制数（0和1），然后将这些产生的随机二进制数与 n 个隐单元的输出相乘（与0相乘的隐单元就相当于是被删除了）；这个计算开销相比于神经网络的正常计算是非常小的；
7. dropout的缺点：
   - **损失函数 Loss 不再被明确的定义，每次迭代都会随机删除一部分隐单元**；
8. dropout使用时的策略：由于dropout的作用是防止过拟合，所以
   - 若某一隐层神经元比较少，过拟合不严重，可以调小概率 $p_{drop}$，甚至该层不使用 dropout；
   - 若某一隐层神经元比较多，过拟合严重，可以调大概率 $p_{drop}$；
9. dropout在使用时更多的可能性（也可称作推广）：
   - 可以对某一层或某几层使用dropout，其他层不使用dropout；
   - 可以对不同的层，甚至不同的隐单元分别设置不同的概率 $p_{drop}$，但实际中一般不这么做；
   - 可以对每个梯度更新周期设置不同的概率 $p_{drop}$，但实际中一般不这么做；
   - 现在对每个隐单元的输出乘的是个二值函数（0或1），不一定必须是二值函数，也可以扩展为乘以其他值，比如：对 n 个隐单元，每个隐单元乘上的值是根据均值为0，方差为1的标准正态分布生成的值。此时单从形式上来看和 L1、L2 正则已经非常相似了；不过实际中一般也很少采用这种方式；

## 3.数据增强（EDA）

### 3.1 词汇增强：

- **基于词典进行替换**：从文本中随机选取一个或多个词语，利用同义词词典将这一个或多个词语替换成其同义词；
- **基于词向量进行替换**：从文本中随机选取一个或多个词语，使用预先训练好的词向量，找到在嵌入空间中与这一个或多个词距离最近的词语，进行替换；
- **基于MLM进行替换**：BERT系列模型是通过MLM进行预训练的，所以可以随机将文本中的某一个或者多个词语mask，使用预训练好的BERT系列模型对mask掉的词语进行生成，以此做增强；
- **基于tf-idf进行替换**：文本中tf-idf较小的词语，对该文本的贡献较小。可以优先对这些词语进行增强，以避免模型错误的将这些词语作为具体任务的主要判断依据；
- **随机插入词语**：向文本的任意位置以一定的概率随机插入词语；
- **随机删除词语**：对文本中任意词语以一定的概率随机进行删除；
- **随机替换词语**：将文本中任意的某一个或多个词语，随机的替换为另外的随机词语；

### 3.2 回译增强：

利用机器翻译模型，或百度翻译、谷歌翻译等，将中文文本翻译为其他语种的文本，然后再翻译回中文文本，以此进行增强；

### 3.3 基于语法解析增强

首先使用语法解析工具解析并生成原始文本的依存关系树，之后按照依存关系对其调整，生成增强后的文本。比如：将文本从一个主动语态调整为被动语态；

### 3.4 直接在输入层注入随机噪声；

## 4.早停（early stopping）

在训练时，一般来说是训练集的损失一直下降，而验证集的损失是先下降后上升的。验证集的损失之所以上升就是因为模型已经在训练集上过拟合了。

选取验证集损失最小的模型，而不是最新的模型，这种策略就是早停。

## 5.对抗训练

详情见 [http://mingchao.wang/Kw0SoUUq/](http://mingchao.wang/Kw0SoUUq/ "http://mingchao.wang/Kw0SoUUq/")

## Reference[#](#reference "#")

- [https://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/3\_regularization.html](https://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/3_regularization.html "https://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/3_regularization.html")
